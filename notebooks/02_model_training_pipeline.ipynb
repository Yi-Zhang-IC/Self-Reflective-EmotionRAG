{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import Sampler\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import optuna\n",
    "from typing import Union, Optional\n",
    "import torch.nn.init as init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import Sampler\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import optuna\n",
    "from typing import Union, Optional\n",
    "import torch.nn.init as init\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch import amp\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch import amp\n",
    "import logging\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "12.4\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Determine the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vipuser/miniconda3/envs/emoenv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2Model(\n",
       "  (embeddings): DebertaV2Embeddings(\n",
       "    (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): DebertaV2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DebertaV2Layer(\n",
       "        (attention): DebertaV2Attention(\n",
       "          (self): DisentangledSelfAttention(\n",
       "            (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): DebertaV2SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DebertaV2Intermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DebertaV2Output(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "# Initialize the model\n",
    "base_model = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "base_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Go emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vipuser/miniconda3/envs/emoenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 0. Import\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"train_single_label_count\": 36308,\n",
      "    \"validation_single_label_count\": 4548,\n",
      "    \"test_single_label_count\": 4590,\n",
      "    \"train_class_distribution\": {\n",
      "        \"0\": 2710,\n",
      "        \"1\": 1652,\n",
      "        \"2\": 1025,\n",
      "        \"3\": 1451,\n",
      "        \"4\": 1873,\n",
      "        \"5\": 649,\n",
      "        \"6\": 858,\n",
      "        \"7\": 1389,\n",
      "        \"8\": 389,\n",
      "        \"9\": 709,\n",
      "        \"10\": 1402,\n",
      "        \"11\": 498,\n",
      "        \"12\": 203,\n",
      "        \"13\": 510,\n",
      "        \"14\": 430,\n",
      "        \"15\": 1857,\n",
      "        \"16\": 39,\n",
      "        \"17\": 853,\n",
      "        \"18\": 1427,\n",
      "        \"19\": 85,\n",
      "        \"20\": 861,\n",
      "        \"21\": 51,\n",
      "        \"22\": 586,\n",
      "        \"23\": 88,\n",
      "        \"24\": 353,\n",
      "        \"25\": 817,\n",
      "        \"26\": 720,\n",
      "        \"27\": 12823\n",
      "    },\n",
      "    \"validation_num_classes\": 28,\n",
      "    \"validation_class_distribution\": {\n",
      "        \"0\": 326,\n",
      "        \"1\": 208,\n",
      "        \"2\": 109,\n",
      "        \"3\": 164,\n",
      "        \"4\": 258,\n",
      "        \"5\": 96,\n",
      "        \"6\": 102,\n",
      "        \"7\": 164,\n",
      "        \"8\": 52,\n",
      "        \"9\": 91,\n",
      "        \"10\": 212,\n",
      "        \"11\": 61,\n",
      "        \"12\": 20,\n",
      "        \"13\": 52,\n",
      "        \"14\": 58,\n",
      "        \"15\": 261,\n",
      "        \"16\": 6,\n",
      "        \"17\": 106,\n",
      "        \"18\": 173,\n",
      "        \"19\": 8,\n",
      "        \"20\": 119,\n",
      "        \"21\": 9,\n",
      "        \"22\": 74,\n",
      "        \"23\": 8,\n",
      "        \"24\": 40,\n",
      "        \"25\": 84,\n",
      "        \"26\": 95,\n",
      "        \"27\": 1592\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load raw GoEmotions dataset\n",
    "# -------------------------------\n",
    "raw_datasets = load_dataset(\"go_emotions\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Filter to single-label samples (train/validation/test separately)\n",
    "# -------------------------------\n",
    "def filter_single_label(split):\n",
    "    filtered = []\n",
    "    for example in raw_datasets[split]:\n",
    "        labels = example[\"labels\"]\n",
    "        if isinstance(labels, list) and len(labels) == 1:\n",
    "            filtered.append({\"text\": example[\"text\"], \"label\": labels[0]})\n",
    "    return filtered\n",
    "\n",
    "filtered_train = filter_single_label(\"train\")\n",
    "filtered_validation = filter_single_label(\"validation\")\n",
    "filtered_test = filter_single_label(\"test\")\n",
    "\n",
    "# For statistics\n",
    "filtered_train_labels = [sample[\"label\"] for sample in filtered_train]\n",
    "class_counter = Counter(filtered_train_labels)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Save the three filtered splits as JSONL\n",
    "# -------------------------------\n",
    "save_dir = Path(\"..\") / \"data\" / \"splits_single_label\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_jsonl(data, name):\n",
    "    with open(save_dir / f\"{name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "save_jsonl(filtered_train, \"train\")\n",
    "save_jsonl(filtered_validation, \"validation\")\n",
    "save_jsonl(filtered_test, \"test\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Output some confirmation\n",
    "# -------------------------------\n",
    "{\n",
    "    \"train_single_label_count\": len(filtered_train),\n",
    "    \"validation_single_label_count\": len(filtered_validation),\n",
    "    \"test_single_label_count\": len(filtered_test),\n",
    "    \"train_class_distribution\": dict(sorted(class_counter.items()))\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Compute validation set statistics\n",
    "# -------------------------------\n",
    "# Count class distribution in the validation set\n",
    "filtered_validation_labels = [sample[\"label\"] for sample in filtered_validation]\n",
    "validation_class_counter = Counter(filtered_validation_labels)\n",
    "\n",
    "# Number of unique classes in the validation set\n",
    "num_classes_validation = len(validation_class_counter)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Output updated confirmation including validation class stats\n",
    "# -------------------------------\n",
    "output_stats = {\n",
    "    \"train_single_label_count\": len(filtered_train),\n",
    "    \"validation_single_label_count\": len(filtered_validation),\n",
    "    \"test_single_label_count\": len(filtered_test),\n",
    "    \"train_class_distribution\": dict(sorted(class_counter.items())),\n",
    "    \"validation_num_classes\": num_classes_validation,\n",
    "    \"validation_class_distribution\": dict(sorted(validation_class_counter.items()))\n",
    "}\n",
    "\n",
    "print(json.dumps(output_stats, indent=4, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Union, Optional\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, path: Union[Path, str], tokenizer, split: Optional[str] = None, max_length: int = 128):\n",
    "        self.samples = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                if split is None or item[\"split\"] == split:\n",
    "                    self.samples.append(item)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels = [s[\"label\"] for s in self.samples]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        enc = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": item[\"label\"]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "# === Fix the paths for notebook ===\n",
    "model_path = Path(\"../model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset_path = Path(\"../data/augmented_single_label.jsonl\")\n",
    "\n",
    "train_dataset = EmotionDataset(dataset_path, tokenizer, split=\"train\")\n",
    "val_dataset = EmotionDataset(dataset_path, tokenizer, split=\"validation\")\n",
    "\n",
    "train_labels = train_dataset.get_labels()\n",
    "val_labels = val_dataset.get_labels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Agmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART paraphraser already exists at d:\\Documents\\FYP\\emotion-retrieval-embeddings\\models\\bart_paraphraser. Skipping download and save.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Compute correct save path\n",
    "# -------------------------------\n",
    "# Go up one level from notebook folder\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "model_save_path = os.path.join(project_root, \"models\", \"bart_paraphraser\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Check if model already exists\n",
    "# -------------------------------\n",
    "if os.path.exists(model_save_path) and os.path.isdir(model_save_path):\n",
    "    print(f\"BART paraphraser already exists at {model_save_path}. Skipping download and save.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Model not found at {model_save_path}. Downloading and saving...\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Download and save\n",
    "    # -------------------------------\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"eugenesiow/bart-paraphrase\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"eugenesiow/bart-paraphrase\")\n",
    "\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "    print(f\"Model successfully saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minority classes (less than 300 samples): [12, 23, 21, 16, 19]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define minority classes\n",
    "minority_class_ids = [class_id for class_id, count in class_counter.items() if count < 300]\n",
    "\n",
    "print(f\"Minority classes (less than 300 samples): {minority_class_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full agmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minority Classes to Augment: [12, 23, 21, 16, 19]\n",
      "\n",
      "Samples and paraphrases needed per minority class:\n",
      "Class ID   Samples to Generate  Paraphrases per Original Sample\n",
      "12         97                   1                             \n",
      "23         212                  3                             \n",
      "21         249                  5                             \n",
      "16         261                  7                             \n",
      "19         215                  3                             \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Find minority classes\n",
    "# -------------------------------\n",
    "\n",
    "# Assume:\n",
    "# - `filtered_labels` = list of ground-truth labels after filtering (single-label)\n",
    "# - `target_samples_per_class` = target number of samples per class\n",
    "\n",
    "target_samples_per_class = 300\n",
    "num_classes = 28\n",
    "\n",
    "# Find classes that are minority (less than target)\n",
    "minority_classes = [class_id for class_id, count in class_counter.items() if count < target_samples_per_class]\n",
    "\n",
    "print(\"Minority Classes to Augment:\", minority_classes)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Calculate generation goals\n",
    "# -------------------------------\n",
    "\n",
    "# For each minority class:\n",
    "# - How many samples to generate\n",
    "# - How many paraphrases needed per original sample\n",
    "class_to_generation_goal = {}\n",
    "class_to_paraphrase_per_sample = {}\n",
    "\n",
    "for class_id in minority_classes:\n",
    "    current_count = class_counter[class_id]\n",
    "    needed_count = target_samples_per_class - current_count\n",
    "\n",
    "    # Number of paraphrases needed per sample\n",
    "    # r_c = ceil(needed / existing)\n",
    "    paraphrases_per_sample = math.ceil(needed_count / current_count)\n",
    "\n",
    "    class_to_generation_goal[class_id] = needed_count\n",
    "    class_to_paraphrase_per_sample[class_id] = paraphrases_per_sample\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Print planning table\n",
    "# -------------------------------\n",
    "\n",
    "print(\"\\nSamples and paraphrases needed per minority class:\")\n",
    "print(\"{:<10} {:<20} {:<30}\".format(\"Class ID\", \"Samples to Generate\", \"Paraphrases per Original Sample\"))\n",
    "\n",
    "for class_id in minority_classes:\n",
    "    print(\"{:<10} {:<20} {:<30}\".format(\n",
    "        class_id,\n",
    "        class_to_generation_goal[class_id],\n",
    "        class_to_paraphrase_per_sample[class_id]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_text(text, num_return_sequences, paraphraser_pipeline):\n",
    "    \"\"\"\n",
    "    Generate paraphrases for a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to paraphrase.\n",
    "        num_return_sequences (int): How many paraphrased versions to generate.\n",
    "        paraphraser_pipeline (transformers.Pipeline): A HuggingFace pipeline for text2text-generation.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of generated paraphrased texts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate paraphrases\n",
    "        outputs = paraphraser_pipeline(\n",
    "            text,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_return_sequences + 4,  # +4 beams to increase diversity\n",
    "            max_length=128,\n",
    "            do_sample=False  # Deterministic decoding (no randomness)\n",
    "        )\n",
    "\n",
    "        # Extract generated texts\n",
    "        paraphrased_texts = [o['generated_text'] for o in outputs]\n",
    "        return paraphrased_texts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Paraphrasing failed: {e}\")\n",
    "        return []  # Return empty list if something goes wrong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load paraphraser model\n",
    "paraphraser = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"eugenesiow/bart-paraphrase\",\n",
    "    device=0  # or device=-1 if you are on CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes selected for augmentation:\n",
      "Class 12: 203 → 300 using 1x per sample\n",
      "Class 23: 88 → 300 using 3x per sample\n",
      "Class 21: 51 → 300 using 5x per sample\n",
      "Class 16: 39 → 300 using 7x per sample\n",
      "Class 19: 85 → 300 using 3x per sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Augmenting Class 12 ---\n",
      "Samples available: 203, generating 1 per sample → 203 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Augmenting Class 23 ---\n",
      "Samples available: 88, generating 3 per sample → 264 total\n",
      "\n",
      "--- Augmenting Class 21 ---\n",
      "Samples available: 51, generating 5 per sample → 255 total\n",
      "\n",
      "--- Augmenting Class 16 ---\n",
      "Samples available: 39, generating 7 per sample → 273 total\n",
      "\n",
      "--- Augmenting Class 19 ---\n",
      "Samples available: 85, generating 3 per sample → 255 total\n",
      "Augmentation complete. Total new augmented samples: 1250\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load filtered training data\n",
    "# -------------------------------\n",
    "train_path = Path(\"..\") / \"data\" / \"splits_single_label\" / \"train.jsonl\"\n",
    "\n",
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    filtered_train = [json.loads(line) for line in f]\n",
    "\n",
    "filtered_train_labels = [sample[\"label\"] for sample in filtered_train]\n",
    "train_class_counter = Counter(filtered_train_labels)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Determine augmentation plan\n",
    "# -------------------------------\n",
    "target_samples_per_class = 300\n",
    "minority_classes = [c for c, count in train_class_counter.items() if count < target_samples_per_class]\n",
    "\n",
    "class_to_paraphrase_per_sample = {\n",
    "    c: math.ceil((target_samples_per_class - train_class_counter[c]) / train_class_counter[c])\n",
    "    for c in minority_classes\n",
    "}\n",
    "\n",
    "print(\"\\nClasses selected for augmentation:\")\n",
    "for c in minority_classes:\n",
    "    print(f\"Class {c}: {train_class_counter[c]} → {target_samples_per_class} using {class_to_paraphrase_per_sample[c]}x per sample\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load paraphraser\n",
    "# -------------------------------\n",
    "paraphraser = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"eugenesiow/bart-paraphrase\",\n",
    "    device=0  # or -1 for CPU\n",
    ")\n",
    "\n",
    "def paraphrase_text(text, num_return_sequences, paraphraser_pipeline):\n",
    "    try:\n",
    "        outputs = paraphraser_pipeline(\n",
    "            text,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_return_sequences + 4,\n",
    "            max_length=128,\n",
    "            do_sample=False\n",
    "        )\n",
    "        return [o['generated_text'] for o in outputs]\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Skipped due to: {e}\")\n",
    "        return []\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Perform augmentation\n",
    "# -------------------------------\n",
    "augmented_raw_samples = []\n",
    "\n",
    "for class_id in minority_classes:\n",
    "    print(f\"\\n--- Augmenting Class {class_id} ---\")\n",
    "    original_samples = [s for s in filtered_train if s[\"label\"] == class_id]\n",
    "    r_c = class_to_paraphrase_per_sample[class_id]\n",
    "\n",
    "    print(f\"Samples available: {len(original_samples)}, generating {r_c} per sample → {len(original_samples) * r_c} total\")\n",
    "\n",
    "    for sample in original_samples:\n",
    "        raw_text = sample[\"text\"]\n",
    "        paraphrased = paraphrase_text(raw_text, r_c, paraphraser)\n",
    "\n",
    "        for p_text in paraphrased:\n",
    "            augmented_raw_samples.append({\n",
    "                \"text\": p_text,\n",
    "                \"label\": class_id\n",
    "            })\n",
    "\n",
    "print(f\"Augmentation complete. Total new augmented samples: {len(augmented_raw_samples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Viewing 8 randomly picked augmented samples per minority class ===\n",
      "\n",
      "--- Class 12 ---\n",
      "Sample 1: I'm so uncomfortable.\n",
      "Sample 2: *people’s ..this is awkward\n",
      "Sample 3: It's embarrassing to share a license with them, that's for sure.\n",
      "Sample 4: *concussion* *Trophy system kills it* *Another concussion* * Trophy System kills it * \"Yo guys watch me throw this cluster\" Honestly embarrassing \n",
      "Sample 5: Please tell me you were buying something extremely embarassing!\n",
      "Sample 6: Nobody said they don't have the right. It's still embarrassing.\n",
      "Sample 7: [NAME] type awkwardness that some thots go nuts for.\n",
      "Sample 8: Those women's marches are honestly embarrasing. Most of the people there only seem to be feminists because it's trendy.\n",
      "\n",
      "--- Class 16 ---\n",
      "Sample 1: Even today, many years later, that still gives me chills. RIP\n",
      "Sample 2: Well that’s not what was said he said drugs that kill you quicker than your meant to die, nicotine fits that description.\n",
      "Sample 3: They really weren't people, they were slavers. Civilized people should have only one answer to that sort of behaviour: violence and death.\n",
      "Sample 4: Youve never played against a good support player then\n",
      "Sample 5: youve never played against a good support then\n",
      "Sample 6: Well that’s not what was said he said drugs that kill you faster than your meant to die. Nicotine fits that description.\n",
      "Sample 7: Ah, this may explain all the untimely heart attack deaths we hear about for those who oppose the state or whatnot.\n",
      "Sample 8: [NAME] Gun crimes are pretty much unheard of on that side of the pond, isn't it?\n",
      "\n",
      "--- Class 19 ---\n",
      "Sample 1: I'm hoping it's more statistical noise at this point still, but it's definitely worrying.\n",
      "Sample 2: Classic Trap Game... I'm nervous. Hope our boys aren't hungover.\n",
      "Sample 3: I am also anxious that people will be angry or surprised or upset for me never telling people about this before.\n",
      "Sample 4: I couldn't reply to the message I received the other day.\n",
      "Sample 5: Now I'm worried that cat will get salmonella :(\n",
      "Sample 6: I gotta say, that was a well fought period. I was worried a couple times...\n",
      "Sample 7: I already called the county I got married in trying to find out about it but they had nothing on either of us about a divorce.. Im so anxious..\n",
      "Sample 8: JFC, that is depressing.\n",
      "\n",
      "--- Class 21 ---\n",
      "Sample 1: My pride. I am like a roach, unsightly to look at but determined to survive.\n",
      "Sample 2: Everyone hates these posts but when we do post them it's because we are proud of the work we do. Congrats!\n",
      "Sample 3: Not surprised by rudi, the man has been a beast since we got him.\n",
      "Sample 4: I'm proud that this is the first response that I see.\n",
      "Sample 5: I'm so glad they remastered that game, I'm having a blast playing it again.\n",
      "Sample 6: In China, innovation means \"I pridefully made this.\"\n",
      "Sample 7: New Year, New You. I'm proud of you.\n",
      "Sample 8: In my school we have specific councilors for the students matched alphabetically. Mine just sucks.\n",
      "\n",
      "--- Class 23 ---\n",
      "Sample 1: No one comes here it's cool\n",
      "Sample 2: Now let's sit down, relax and have a nice cool glass of turnip juice.\n",
      "Sample 3: At least she's not a dirty hamster.\n",
      "Sample 4: Sport shooting can be done with air guns. Problem solved.\n",
      "Sample 5: Finally! Somebody who legitimately hates planet Jupiter!\n",
      "Sample 6: Dude was okay! Oh thank god, I figured at best paralyzed from waist down.\n",
      "Sample 7: Glad your SO was helpful and hope you are better.\n",
      "Sample 8: I heard someone say one... `` everywhere is better but Perth is the best! Glad you enjoyed your holiday!\n"
     ]
    }
   ],
   "source": [
    "# View random samples per class\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class_to_augmented = defaultdict(list)\n",
    "\n",
    "for sample in augmented_raw_samples:\n",
    "    class_to_augmented[sample['label']].append(sample['text'])\n",
    "\n",
    "print(\"\\n=== Viewing 8 randomly picked augmented samples per minority class ===\")\n",
    "\n",
    "for class_id in sorted(minority_classes):\n",
    "    print(f\"\\n--- Class {class_id} ---\")\n",
    "    samples = random.sample(class_to_augmented[class_id], k=min(8, len(class_to_augmented[class_id])))\n",
    "    for i, s in enumerate(samples, 1):\n",
    "        print(f\"Sample {i}: {s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge the original + augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged total: 46696 (Train: 37558, Val: 4548, Test: 4590)\n",
      "Merged dataset with splits saved to: D:\\Documents\\FYP\\emotion-retrieval-embeddings\\data\\augmented_single_label.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load validation and test splits\n",
    "# -------------------------------\n",
    "val_path = Path(\"..\") / \"data\" / \"splits_single_label\" / \"validation.jsonl\"\n",
    "test_path = Path(\"..\") / \"data\" / \"splits_single_label\" / \"test.jsonl\"\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "filtered_validation = load_jsonl(val_path)\n",
    "filtered_test = load_jsonl(test_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Annotate splits and merge\n",
    "# -------------------------------\n",
    "\n",
    "# Add split tags\n",
    "train_all = [{\"text\": s[\"text\"], \"label\": s[\"label\"], \"split\": \"train\"} for s in filtered_train + augmented_raw_samples]\n",
    "validation_all = [{\"text\": s[\"text\"], \"label\": s[\"label\"], \"split\": \"validation\"} for s in filtered_validation]\n",
    "test_all = [{\"text\": s[\"text\"], \"label\": s[\"label\"], \"split\": \"test\"} for s in filtered_test]\n",
    "\n",
    "merged_dataset = train_all + validation_all + test_all\n",
    "\n",
    "print(f\"Merged total: {len(merged_dataset)} (Train: {len(train_all)}, Val: {len(validation_all)}, Test: {len(test_all)})\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Save as JSONL\n",
    "# -------------------------------\n",
    "save_path = Path(\"..\") / \"data\" / \"augmented_single_label.jsonl\"\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in merged_dataset:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Merged dataset with splits saved to: {save_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a balanced batch sampler for single-label samples\n",
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class UniformBalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, num_classes, labels):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.labels = labels\n",
    "\n",
    "        # Build mapping: class -> list of indices\n",
    "        self.class_to_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            self.class_to_indices[label].append(idx)\n",
    "\n",
    "        self.available_classes = [c for c in range(self.num_classes) if len(self.class_to_indices[c]) > 0]\n",
    "\n",
    "        if self.batch_size < len(self.available_classes):\n",
    "            raise ValueError(f\"Batch size must be >= number of available classes ({len(self.available_classes)}). Got {self.batch_size}.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            batch_indices = []\n",
    "\n",
    "            # Sample one from each available class\n",
    "            for class_id in self.available_classes:\n",
    "                candidates = self.class_to_indices[class_id]\n",
    "                if candidates:\n",
    "                    selected = random.choice(candidates)\n",
    "                    batch_indices.append(selected)\n",
    "\n",
    "            # Fill remaining slots randomly\n",
    "            while len(batch_indices) < self.batch_size:\n",
    "                class_id = random.choice(self.available_classes)\n",
    "                candidates = self.class_to_indices[class_id]\n",
    "                selected = random.choice(candidates)\n",
    "                batch_indices.append(selected)\n",
    "\n",
    "            random.shuffle(batch_indices)\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1000000  # or some large number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BSCLossSingleLabel(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(BSCLossSingleLabel, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Tensor of shape (batch_size, hidden_dim) - embeddings\n",
    "            labels: Tensor of shape (batch_size,) - single integer label per sample\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Normalize features to unit hypersphere\n",
    "        features = F.normalize(features, p=2, dim=1)\n",
    "\n",
    "        # Compute similarity matrix (batch_size x batch_size)\n",
    "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "\n",
    "        # Positive mask: label[i] == label[j], and i != j\n",
    "        labels = labels.view(-1, 1)  # shape (batch_size, 1)\n",
    "        positive_mask = (labels == labels.T).float()  # (batch_size, batch_size)\n",
    "        diag_mask = torch.eye(batch_size, device=device)\n",
    "        positive_mask = positive_mask * (1 - diag_mask)  # remove diagonal\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            pos_indices = positive_mask[i].nonzero(as_tuple=False).squeeze(1)\n",
    "\n",
    "            if len(pos_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            numerator = torch.exp(sim_matrix[i, pos_indices])\n",
    "\n",
    "            denominator = 0.0\n",
    "            for c in torch.unique(labels):\n",
    "                class_indices = (labels.squeeze() == c).nonzero(as_tuple=False).squeeze(1)\n",
    "                class_indices = class_indices[class_indices != i]\n",
    "\n",
    "                if len(class_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                class_sims = torch.exp(sim_matrix[i, class_indices])\n",
    "                class_sum = class_sims.sum()\n",
    "                class_sum = class_sum / len(class_indices)\n",
    "\n",
    "                denominator += class_sum\n",
    "\n",
    "            loss_i = - torch.mean(torch.log(numerator / denominator))\n",
    "            losses.append(loss_i)\n",
    "\n",
    "        if len(losses) == 0:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "\n",
    "        return torch.mean(torch.stack(losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test my loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Detailed BSCL Diagnostic ===\n",
      "\n",
      "Similarity Matrix:\n",
      "tensor([[10.0000, -1.2799, -4.4199,  7.7487, -3.4071, -0.1243, -8.3086,  7.1075],\n",
      "        [-1.2799, 10.0000,  1.6897,  0.6522, -2.5841,  1.6311,  5.5117, -3.7046],\n",
      "        [-4.4199,  1.6897, 10.0000, -6.9804, -6.4275, -6.7476,  3.7009, -0.7663],\n",
      "        [ 7.7487,  0.6522, -6.9804, 10.0000,  0.5539,  5.7648, -4.0981,  6.2141],\n",
      "        [-3.4071, -2.5841, -6.4275,  0.5539, 10.0000,  5.6240,  1.7582, -4.3310],\n",
      "        [-0.1243,  1.6311, -6.7476,  5.7648,  5.6240, 10.0000,  3.1521,  0.3289],\n",
      "        [-8.3086,  5.5117,  3.7009, -4.0981,  1.7582,  3.1521, 10.0000, -5.4888],\n",
      "        [ 7.1075, -3.7046, -0.7663,  6.2141, -4.3310,  0.3289, -5.4888, 10.0000]])\n",
      "\n",
      "Positive Mask (1 = same class, 0 = other):\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "\n",
      "Class-wise indices:\n",
      "Class 0: indices [0, 1]\n",
      "Class 1: indices [2, 3]\n",
      "Class 2: indices [4, 5]\n",
      "Class 3: indices [6, 7]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def print_sim_matrix_info(features, labels, temperature=0.1):\n",
    "    print(\"\\n=== Detailed BSCL Diagnostic ===\")\n",
    "    features = F.normalize(features, p=2, dim=1)\n",
    "    sim_matrix = torch.matmul(features, features.T) / temperature\n",
    "\n",
    "    print(\"\\nSimilarity Matrix:\")\n",
    "    print(sim_matrix)\n",
    "\n",
    "    labels = labels.view(-1, 1)\n",
    "    batch_size = labels.size(0)\n",
    "    pos_mask = (labels == labels.T).float() - torch.eye(batch_size)\n",
    "    print(\"\\nPositive Mask (1 = same class, 0 = other):\")\n",
    "    print(pos_mask)\n",
    "\n",
    "    print(\"\\nClass-wise indices:\")\n",
    "    for c in torch.unique(labels):\n",
    "        indices = (labels.squeeze() == c).nonzero(as_tuple=False).squeeze(1)\n",
    "        print(f\"Class {c.item()}: indices {indices.tolist()}\")\n",
    "\n",
    "    return sim_matrix, pos_mask\n",
    "\n",
    "features = F.normalize(torch.randn(8, 5), dim=1)  # Random 8x5 features\n",
    "labels = torch.tensor([0, 0, 1, 1, 2, 2, 3, 3])   # 4 classes, 2 samples each\n",
    "\n",
    "sim_matrix, pos_mask = print_sim_matrix_info(features, labels, temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Similarity Matrix ===\n",
      "tensor([[ 10.0000,   9.9400,   0.0000,   0.0000,   0.0000,   1.1000, -10.0000,\n",
      "          -9.9400],\n",
      "        [  9.9400,  10.0000,   1.1000,   1.1000,   0.0000,   1.1000,  -9.9400,\n",
      "         -10.0000],\n",
      "        [  0.0000,   1.1000,  10.0000,   9.9400,   0.0000,   0.0000,   0.0000,\n",
      "          -1.1000],\n",
      "        [  0.0000,   1.1000,   9.9400,  10.0000,   1.1000,   1.1000,   0.0000,\n",
      "          -1.1000],\n",
      "        [  0.0000,   0.0000,   0.0000,   1.1000,  10.0000,   9.9400,   0.0000,\n",
      "           0.0000],\n",
      "        [  1.1000,   1.1000,   0.0000,   1.1000,   9.9400,  10.0000,  -1.1000,\n",
      "          -1.1000],\n",
      "        [-10.0000,  -9.9400,   0.0000,   0.0000,   0.0000,  -1.1000,  10.0000,\n",
      "           9.9400],\n",
      "        [ -9.9400, -10.0000,  -1.1000,  -1.1000,   0.0000,  -1.1000,   9.9400,\n",
      "          10.0000]])\n",
      "\n",
      "=== Positive Mask ===\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "\n",
      "=== Positive Indices per Sample ===\n",
      "Sample 0: Positives -> [1], Class: 0\n",
      "Sample 1: Positives -> [0], Class: 0\n",
      "Sample 2: Positives -> [3], Class: 1\n",
      "Sample 3: Positives -> [2], Class: 1\n",
      "Sample 4: Positives -> [5], Class: 2\n",
      "Sample 5: Positives -> [4], Class: 2\n",
      "Sample 6: Positives -> [7], Class: 3\n",
      "Sample 7: Positives -> [6], Class: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a dummy 8-vector batch with 4 classes (2 per class)\n",
    "x = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],    # Class 0\n",
    "    [0.9, 0.1, 0.0],    # Class 0\n",
    "    [0.0, 1.0, 0.0],    # Class 1\n",
    "    [0.0, 0.9, 0.1],    # Class 1\n",
    "    [0.0, 0.0, 1.0],    # Class 2\n",
    "    [0.1, 0.0, 0.9],    # Class 2\n",
    "    [-1.0, 0.0, 0.0],   # Class 3\n",
    "    [-0.9, -0.1, 0.0],  # Class 3\n",
    "])\n",
    "y = torch.tensor([0, 0, 1, 1, 2, 2, 3, 3])\n",
    "\n",
    "# Normalize\n",
    "x_norm = F.normalize(x, p=2, dim=1)\n",
    "sim_matrix = torch.matmul(x_norm, x_norm.T) / 0.1\n",
    "\n",
    "# Compute positive mask\n",
    "labels = y.view(-1, 1)\n",
    "positive_mask = (labels == labels.T).float()\n",
    "diag = torch.eye(len(y))\n",
    "positive_mask = positive_mask * (1 - diag)\n",
    "\n",
    "# Print diagnostics\n",
    "print(\"=== Similarity Matrix ===\")\n",
    "print(sim_matrix.round(decimals=2))  # FIXED: use decimals=2 instead of positional arg\n",
    "\n",
    "print(\"\\n=== Positive Mask ===\")\n",
    "print(positive_mask)\n",
    "\n",
    "print(\"\\n=== Positive Indices per Sample ===\")\n",
    "for i in range(len(y)):\n",
    "    pos_indices = (positive_mask[i] > 0).nonzero(as_tuple=False).squeeze(1).tolist()\n",
    "    print(f\"Sample {i}: Positives -> {pos_indices}, Class: {y[i].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9187521934509277"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# Simulate realistic batch of features from a small model\n",
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "feature_dim = 64\n",
    "num_classes = 8\n",
    "\n",
    "# Randomly generate normalized feature vectors\n",
    "features = F.normalize(torch.randn(batch_size, feature_dim), p=2, dim=1)\n",
    "\n",
    "# Randomly assign labels (integers from 0 to num_classes-1)\n",
    "labels = torch.randint(low=0, high=num_classes, size=(batch_size,))\n",
    "\n",
    "# Create criterion and compute loss\n",
    "criterion = BSCLossSingleLabel(temperature=0.1)\n",
    "loss_value = criterion(features, labels)\n",
    "\n",
    "loss_value.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning plan\n",
    "1.  Stage 1 creates a strong \"backbone\" embedding space, where emotions are cleanly separated. Filter GoEmotions to only single-label sentences\n",
    "2. Stage 2 teaches the model the complex, messy mixtures that real human feelings exhibit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion_dataset.py\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_length=128):\n",
    "        self.samples = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                if item[\"split\"] == \"train\":  # use only training data\n",
    "                    self.samples.append(item)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels = [s[\"label\"] for s in self.samples]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": item[\"label\"]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_validation_loss(model, dataset, criterion, num_classes=28, batch_size=32, max_steps=30):\n",
    "    model.eval()\n",
    "\n",
    "    # Build balanced sampler\n",
    "    val_labels = dataset.get_labels()\n",
    "    val_sampler = UniformBalancedBatchSampler(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_classes=num_classes,\n",
    "        labels=val_labels\n",
    "    )\n",
    "\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    sampler_iter = iter(val_sampler)\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        batch_indices = next(sampler_iter)\n",
    "        batch = [dataset[i] for i in batch_indices]\n",
    "\n",
    "        input_ids = torch.stack([item[\"input_ids\"] for item in batch]).to(device)\n",
    "        attention_mask = torch.stack([item[\"attention_mask\"] for item in batch]).to(device)\n",
    "        labels = torch.tensor([item[\"label\"] for item in batch]).to(device)\n",
    "\n",
    "        embeddings = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(embeddings, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "\n",
    "    return total_loss / steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer and dataset\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataset_path = \"../data/augmented_single_label.jsonl\"\n",
    "train_dataset = EmotionDataset(dataset_path, tokenizer)\n",
    "train_labels = train_dataset.get_labels()\n",
    "\n",
    "# Load validation data (same tokenizer)\n",
    "val_dataset = EmotionDataset(dataset_path, tokenizer)\n",
    "val_samples = [s for s in val_dataset.samples if s[\"split\"] == \"validation\"]\n",
    "val_dataset.samples = val_samples\n",
    "val_dataset.labels = [s[\"label\"] for s in val_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare balanced sampler and DataLoader\n",
    "num_classes = 28\n",
    "batch_size = 32\n",
    "\n",
    "sampler = UniformBalancedBatchSampler(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_classes=num_classes,\n",
    "    labels=train_labels\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=sampler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionEmbeddingModel(nn.Module):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-base\"):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward through encoder\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use [CLS] token embedding as sentence representation\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0]  # shape: (batch_size, hidden_dim)\n",
    "        return cls_embedding\n",
    "\n",
    "model = EmotionEmbeddingModel().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(hparams, train_dataset, val_dataset, device, num_classes=28, steps_per_epoch=100, val_steps=30):\n",
    "    from torch.utils.data import DataLoader\n",
    "    import torch.nn as nn\n",
    "    import torch\n",
    "\n",
    "    # 1. Setup\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    learning_rate = hparams[\"learning_rate\"]\n",
    "    temperature = hparams[\"temperature\"]\n",
    "\n",
    "    model = EmotionEmbeddingModel().to(device)\n",
    "    criterion = BSCLossSingleLabel(temperature=temperature)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 2. Sampler & Iterators\n",
    "    train_labels = train_dataset.get_labels()\n",
    "    train_sampler = UniformBalancedBatchSampler(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_classes=num_classes,\n",
    "        labels=train_labels\n",
    "    )\n",
    "    train_sampler_iter = iter(train_sampler)\n",
    "\n",
    "    val_labels = val_dataset.get_labels()\n",
    "    val_sampler = UniformBalancedBatchSampler(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_classes=num_classes,\n",
    "        labels=val_labels\n",
    "    )\n",
    "    val_sampler_iter = iter(val_sampler)\n",
    "\n",
    "\n",
    "    # 3. Training Loop\n",
    "    num_epochs = hparams.get(\"num_epochs\", 3)\n",
    "    print_every = hparams.get(\"print_every\", 10)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        print(f\"\\n==== Epoch {epoch + 1} ====\")\n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_indices = next(train_sampler_iter)\n",
    "            batch = [train_dataset[i] for i in batch_indices]\n",
    "\n",
    "            input_ids = torch.stack([item[\"input_ids\"] for item in batch]).to(device)\n",
    "            attention_mask = torch.stack([item[\"attention_mask\"] for item in batch]).to(device)\n",
    "            labels = torch.tensor([item[\"label\"] for item in batch]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if step % print_every == 0:\n",
    "                print(f\"Step {step:03d} | Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_loss / steps_per_epoch\n",
    "        avg_val_loss = compute_validation_loss(model, val_sampler_iter, val_dataset, criterion, steps=val_steps)\n",
    "        print(f\"Epoch {epoch + 1} — Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return avg_val_loss  # Use for Optuna or score-based model selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization (BO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 15:32:19,548] A new study created in memory with name: no-name-6578b0a6-ac14-4b03-b2ff-6c0ef9f82521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Epoch 1 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-16 15:32:36,291] Trial 0 failed with parameters: {'learning_rate': 5.628012622199612e-06, 'temperature': 0.08550502296170108, 'batch_size': 128} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 355.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)').\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\hello\\AppData\\Local\\Temp\\ipykernel_33456\\1317052419.py\", line 12, in objective\n",
      "    val_loss = train_and_evaluate(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\hello\\AppData\\Local\\Temp\\ipykernel_33456\\661948904.py\", line 53, in train_and_evaluate\n",
      "    embeddings = model(input_ids=input_ids, attention_mask=attention_mask)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\hello\\AppData\\Local\\Temp\\ipykernel_33456\\2872696463.py\", line 11, in forward\n",
      "    outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 874, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 674, in forward\n",
      "    output_states, attn_weights = layer_module(\n",
      "                                  ^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 442, in forward\n",
      "    attention_output, att_matrix = self.attention(\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 375, in forward\n",
      "    self_output, att_matrix = self.self(\n",
      "                              ^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 255, in forward\n",
      "    rel_att = self.disentangled_attention_bias(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 309, in disentangled_attention_bias\n",
      "    ).repeat(query_layer.size(0) // self.num_attention_heads, 1, 1)\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 355.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[W 2025-04-16 15:32:36,334] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 355.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m val_loss\n\u001b[32m     21\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Final results\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobjective\u001b[39m(trial):\n\u001b[32m      4\u001b[39m     hparams = {\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m\"\u001b[39m: trial.suggest_float(\u001b[33m\"\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1e-6\u001b[39m, \u001b[32m5e-5\u001b[39m, log=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: trial.suggest_float(\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.03\u001b[39m, \u001b[32m0.2\u001b[39m, log=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprint_every\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m10\u001b[39m\n\u001b[32m     10\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     val_loss = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m val_loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(hparams, train_dataset, val_dataset, device, num_classes, steps_per_epoch, val_steps)\u001b[39m\n\u001b[32m     50\u001b[39m labels = torch.tensor([item[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]).to(device)\n\u001b[32m     52\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m embeddings = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m loss = criterion(embeddings, labels)\n\u001b[32m     55\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mEmotionEmbeddingModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Forward through encoder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Use [CLS] token embedding as sentence representation\u001b[39;00m\n\u001b[32m     13\u001b[39m     cls_embedding = outputs.last_hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# shape: (batch_size, hidden_dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:874\u001b[39m, in \u001b[36mDebertaV2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    864\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m    866\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    867\u001b[39m     input_ids=input_ids,\n\u001b[32m    868\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    871\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    872\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m874\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.z_steps > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:674\u001b[39m, in \u001b[36mDebertaV2Encoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[39m\n\u001b[32m    664\u001b[39m     output_states, attn_weights = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    665\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    666\u001b[39m         next_kv,\n\u001b[32m   (...)\u001b[39m\u001b[32m    671\u001b[39m         output_attentions,\n\u001b[32m    672\u001b[39m     )\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     output_states, attn_weights = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    684\u001b[39m     all_attentions = all_attentions + (attn_weights,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:442\u001b[39m, in \u001b[36mDebertaV2Layer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    435\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    441\u001b[39m ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     attention_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m    451\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:375\u001b[39m, in \u001b[36mDebertaV2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    367\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    368\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m     rel_embeddings=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    374\u001b[39m ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     self_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m         query_states = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:255\u001b[39m, in \u001b[36mDisentangledSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relative_attention:\n\u001b[32m    254\u001b[39m     rel_embeddings = \u001b[38;5;28mself\u001b[39m.pos_dropout(rel_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     rel_att = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    260\u001b[39m     attention_scores = attention_scores + rel_att\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:309\u001b[39m, in \u001b[36mDisentangledSelfAttention.disentangled_attention_bias\u001b[39m\u001b[34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[39m\n\u001b[32m    305\u001b[39m rel_embeddings = rel_embeddings[\u001b[32m0\u001b[39m : att_span * \u001b[32m2\u001b[39m, :].unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.share_att_key:\n\u001b[32m    307\u001b[39m     pos_query_layer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtranspose_for_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_attention_heads\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m     pos_key_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28mself\u001b[39m.key_proj(rel_embeddings), \u001b[38;5;28mself\u001b[39m.num_attention_heads).repeat(\n\u001b[32m    311\u001b[39m         query_layer.size(\u001b[32m0\u001b[39m) // \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m\n\u001b[32m    312\u001b[39m     )\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 355.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    hparams = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 5e-5, log=True),\n",
    "        \"temperature\": trial.suggest_float(\"temperature\", 0.03, 0.2, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "        \"num_epochs\": 3,\n",
    "        \"print_every\": 10\n",
    "    }\n",
    "\n",
    "    val_loss = train_and_evaluate(\n",
    "        hparams=hparams,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Final results\n",
    "print(\"Best trial:\")\n",
    "print(f\"Validation Loss: {study.best_value:.4f}\")\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 45561\n",
      "Validation dataset size: 5426\n",
      "Test dataset size: 5427\n"
     ]
    }
   ],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, path: Union[Path, str], tokenizer, num_classes: int = 28, max_length: int = 256):\n",
    "        self.samples = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                self.samples.append(item)\n",
    "\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_classes = num_classes  # Number of classes for multi-label task\n",
    "        self.labels     = [s[\"labels\"] for s in self.samples]  # Multi-label list of labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        enc = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",  # Ensure padding is done to a fixed max_length\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Handle multi-label padding\n",
    "        label = item[\"labels\"]\n",
    "        label_vector = torch.zeros(self.num_classes)  # Initialize a zero vector of size num_classes\n",
    "        for l in label:\n",
    "            label_vector[l] = 1  # Set the class indices to 1\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),  # Remove the batch dimension\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),  # Remove the batch dimension\n",
    "            \"labels\": label_vector  # Return the multi-label as a binary vector\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "# 1) Point to your local “model” folder\n",
    "model_path = \"../models/roberta-base-go_emotions\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  # Correct tokenizer path\n",
    "\n",
    "# 2) Define paths to your train, validation, and test JSONL files\n",
    "base_dir = Path(\"..\")  \n",
    "dataset_dir = base_dir / \"data\" / \"augmented_go_emotion\"\n",
    "train_path = dataset_dir / \"train.jsonl\"\n",
    "val_path = dataset_dir / \"validation.jsonl\"\n",
    "test_path = dataset_dir / \"test.jsonl\"\n",
    "\n",
    "# 3) Load train/val/test datasets\n",
    "train_dataset = EmotionDataset(train_path, tokenizer)\n",
    "val_dataset   = EmotionDataset(val_path, tokenizer)\n",
    "test_dataset  = EmotionDataset(test_path, tokenizer)\n",
    "\n",
    "# Get labels for each dataset\n",
    "train_labels = train_dataset.get_labels()\n",
    "val_labels   = val_dataset.get_labels()\n",
    "test_labels  = test_dataset.get_labels()\n",
    "\n",
    "# Log the dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProportionalCoverageSampler(Sampler):\n",
    "    def __init__(self, labels, batch_size, num_classes):\n",
    "        self.labels = [lbl if isinstance(lbl, list) else [lbl] for lbl in labels]\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.class_to_indices = defaultdict(list)\n",
    "        for idx, lbls in enumerate(self.labels):\n",
    "            for l in lbls:\n",
    "                self.class_to_indices[l].append(idx)\n",
    "\n",
    "        label_counter = Counter(l for lbls in self.labels for l in lbls)\n",
    "        class_freqs = np.array([label_counter.get(c, 0) for c in range(num_classes)])\n",
    "        self.class_probs = class_freqs / class_freqs.sum()\n",
    "\n",
    "        self.rotation_pool = {\n",
    "            c: self.class_to_indices[c][:] for c in range(num_classes)\n",
    "        }\n",
    "        for c in range(num_classes):\n",
    "            random.shuffle(self.rotation_pool[c])\n",
    "\n",
    "    def _get_next_index(self, c):\n",
    "        pool = self.rotation_pool[c]\n",
    "        if not pool:\n",
    "            pool = self.class_to_indices[c][:]\n",
    "            random.shuffle(pool)\n",
    "            self.rotation_pool[c] = pool\n",
    "        if not pool:\n",
    "            return None\n",
    "        return pool.pop()\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            batch_indices = set()\n",
    "\n",
    "            selected_classes = list(range(self.num_classes))\n",
    "            random.shuffle(selected_classes)\n",
    "\n",
    "            for c in selected_classes:\n",
    "                if len(batch_indices) >= self.batch_size:\n",
    "                    break\n",
    "                idx = self._get_next_index(c)\n",
    "                if idx is not None:\n",
    "                    batch_indices.add(idx)\n",
    "\n",
    "            while len(batch_indices) < self.batch_size:\n",
    "                c = np.random.choice(self.num_classes, p=self.class_probs)\n",
    "                idx = self._get_next_index(c)\n",
    "                if idx is None or idx in batch_indices:\n",
    "                    continue\n",
    "                batch_indices.add(idx)\n",
    "\n",
    "            yield list(batch_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1:\n",
      "  Total examples       : 32\n",
      "  Single-label samples : 20\n",
      "  Multi-label samples  : 12\n",
      "  Label counts         : {0: 3, 1: 2, 2: 2, 3: 3, 4: 1, 5: 2, 6: 2, 7: 1, 8: 2, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 3, 19: 1, 20: 3, 21: 1, 22: 1, 23: 1, 24: 1, 25: 2, 26: 1, 27: 3}\n",
      "\n",
      "Batch 2:\n",
      "  Total examples       : 32\n",
      "  Single-label samples : 18\n",
      "  Multi-label samples  : 14\n",
      "  Label counts         : {0: 2, 1: 2, 2: 2, 3: 1, 4: 2, 5: 1, 6: 2, 7: 3, 8: 2, 9: 2, 10: 1, 11: 2, 12: 1, 13: 1, 14: 1, 15: 2, 16: 2, 17: 2, 18: 2, 19: 2, 20: 2, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 2, 27: 2}\n",
      "\n",
      "Batch 3:\n",
      "  Total examples       : 32\n",
      "  Single-label samples : 18\n",
      "  Multi-label samples  : 14\n",
      "  Label counts         : {0: 4, 1: 1, 2: 1, 3: 2, 4: 6, 5: 1, 6: 2, 7: 1, 8: 1, 9: 3, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 3, 18: 1, 19: 2, 20: 2, 21: 1, 22: 2, 23: 1, 24: 2, 25: 2, 26: 1, 27: 3}\n",
      "\n",
      "Batch 4:\n",
      "  Total examples       : 32\n",
      "  Single-label samples : 17\n",
      "  Multi-label samples  : 15\n",
      "  Label counts         : {0: 2, 1: 2, 2: 1, 3: 3, 4: 2, 5: 1, 6: 1, 7: 3, 8: 1, 9: 3, 10: 3, 11: 1, 12: 1, 13: 2, 14: 1, 15: 3, 16: 1, 17: 1, 18: 1, 19: 2, 20: 3, 21: 2, 22: 2, 23: 1, 24: 1, 25: 3, 26: 2, 27: 3}\n",
      "\n",
      "Batch 5:\n",
      "  Total examples       : 32\n",
      "  Single-label samples : 22\n",
      "  Multi-label samples  : 10\n",
      "  Label counts         : {0: 2, 1: 1, 2: 1, 3: 2, 4: 1, 5: 2, 6: 1, 7: 3, 8: 2, 9: 1, 10: 2, 11: 1, 12: 2, 13: 1, 14: 2, 15: 3, 16: 1, 17: 3, 18: 2, 19: 1, 20: 1, 21: 1, 22: 2, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1}\n",
      "\n",
      "Batch 6:\n",
      "  Total examples       : 32\n",
      "  Single-label samples : 17\n",
      "  Multi-label samples  : 15\n",
      "  Label counts         : {0: 4, 1: 1, 2: 2, 3: 3, 4: 1, 5: 1, 6: 1, 7: 2, 8: 2, 9: 2, 10: 2, 11: 1, 12: 1, 13: 2, 14: 1, 15: 1, 16: 1, 17: 1, 18: 2, 19: 1, 20: 4, 21: 1, 22: 3, 23: 1, 24: 1, 25: 1, 26: 1, 27: 6}\n",
      "\n",
      "Batch 7:\n",
      "  Total examples       : 32\n",
      "  Single-label samples : 18\n",
      "  Multi-label samples  : 14\n",
      "  Label counts         : {0: 4, 1: 1, 2: 2, 3: 2, 4: 2, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 2, 12: 1, 13: 1, 14: 2, 15: 2, 16: 1, 17: 1, 18: 3, 19: 1, 20: 1, 21: 1, 22: 1, 23: 2, 24: 1, 25: 1, 26: 2, 27: 6}\n",
      "\n",
      "Batch 8:\n",
      "  Total examples       : 32\n",
      "  Single-label samples : 21\n",
      "  Multi-label samples  : 11\n",
      "  Label counts         : {0: 2, 1: 1, 2: 1, 3: 1, 4: 2, 5: 2, 6: 1, 7: 2, 8: 2, 9: 2, 10: 3, 11: 1, 12: 1, 13: 2, 14: 1, 15: 2, 16: 2, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 2, 26: 1, 27: 6}\n",
      "\n",
      "Batch 9:\n",
      "  Total examples       : 32\n",
      "  Single-label samples : 23\n",
      "  Multi-label samples  : 9\n",
      "  Label counts         : {0: 1, 1: 1, 2: 2, 3: 2, 4: 5, 5: 1, 6: 1, 7: 2, 8: 1, 9: 2, 10: 1, 11: 2, 12: 1, 13: 1, 14: 1, 15: 1, 16: 2, 17: 1, 18: 2, 19: 1, 20: 1, 21: 1, 22: 2, 23: 1, 24: 2, 25: 1, 26: 2, 27: 2}\n",
      "\n",
      "Batch 10:\n",
      "  Total examples       : 32\n",
      "  Single-label samples : 17\n",
      "  Multi-label samples  : 15\n",
      "  Label counts         : {0: 5, 1: 1, 2: 1, 3: 2, 4: 3, 5: 2, 6: 1, 7: 1, 8: 1, 9: 3, 10: 1, 11: 1, 12: 1, 13: 4, 14: 2, 15: 3, 16: 1, 17: 1, 18: 2, 19: 2, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 2, 27: 3}\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/roberta-base-go_emotions\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "train_path = Path(\"../data/augmented_go_emotion/train.jsonl\")\n",
    "train_dataset = EmotionDataset(train_path, tokenizer)\n",
    "train_labels = train_dataset.get_labels()\n",
    "# Create sampler\n",
    "sampler = ProportionalCoverageSampler(\n",
    "    labels=train_labels,\n",
    "    batch_size=32,\n",
    "    num_classes=28\n",
    ")\n",
    "\n",
    "# Sample 10 batches\n",
    "batches = []\n",
    "for _, batch in zip(range(10), sampler):\n",
    "    batches.append(batch)\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    label_count = Counter()\n",
    "    single_label_count = 0\n",
    "    multi_label_count = 0\n",
    "\n",
    "    for idx in batch:\n",
    "        lbls = train_labels[idx]\n",
    "        for l in lbls:\n",
    "            label_count[l] += 1\n",
    "        if len(lbls) == 1:\n",
    "            single_label_count += 1\n",
    "        else:\n",
    "            multi_label_count += 1\n",
    "\n",
    "    print(f\"\\nBatch {i + 1}:\")\n",
    "    print(f\"  Total examples       : {len(batch)}\")\n",
    "    print(f\"  Single-label samples : {single_label_count}\")\n",
    "    print(f\"  Multi-label samples  : {multi_label_count}\")\n",
    "    print(f\"  Label counts         : {dict(sorted(label_count.items()))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(label_counts_per_batch).fillna(0).astype(int)\n",
    "df.plot(kind=\"bar\", stacked=True, figsize=(12, 6))\n",
    "plt.xlabel(\"Batch Index\")\n",
    "plt.ylabel(\"Label Count\")\n",
    "plt.title(\"Smoothed Label Distribution per Batch (Train Set)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

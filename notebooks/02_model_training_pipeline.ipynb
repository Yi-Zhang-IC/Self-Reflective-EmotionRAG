{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Determine the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\FYP\\emotion-retrieval-embeddings\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2Model(\n",
       "  (embeddings): DebertaV2Embeddings(\n",
       "    (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): DebertaV2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DebertaV2Layer(\n",
       "        (attention): DebertaV2Attention(\n",
       "          (self): DisentangledSelfAttention(\n",
       "            (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): DebertaV2SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DebertaV2Intermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DebertaV2Output(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "# Initialize the model\n",
    "base_model = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "base_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Go emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Import\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_single_label_count': 36308,\n",
       " 'validation_single_label_count': 4548,\n",
       " 'test_single_label_count': 4590,\n",
       " 'train_class_distribution': {0: 2710,\n",
       "  1: 1652,\n",
       "  2: 1025,\n",
       "  3: 1451,\n",
       "  4: 1873,\n",
       "  5: 649,\n",
       "  6: 858,\n",
       "  7: 1389,\n",
       "  8: 389,\n",
       "  9: 709,\n",
       "  10: 1402,\n",
       "  11: 498,\n",
       "  12: 203,\n",
       "  13: 510,\n",
       "  14: 430,\n",
       "  15: 1857,\n",
       "  16: 39,\n",
       "  17: 853,\n",
       "  18: 1427,\n",
       "  19: 85,\n",
       "  20: 861,\n",
       "  21: 51,\n",
       "  22: 586,\n",
       "  23: 88,\n",
       "  24: 353,\n",
       "  25: 817,\n",
       "  26: 720,\n",
       "  27: 12823}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load raw GoEmotions dataset\n",
    "# -------------------------------\n",
    "raw_datasets = load_dataset(\"go_emotions\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Filter to single-label samples (train/validation/test separately)\n",
    "# -------------------------------\n",
    "def filter_single_label(split):\n",
    "    filtered = []\n",
    "    for example in raw_datasets[split]:\n",
    "        labels = example[\"labels\"]\n",
    "        if isinstance(labels, list) and len(labels) == 1:\n",
    "            filtered.append({\"text\": example[\"text\"], \"label\": labels[0]})\n",
    "    return filtered\n",
    "\n",
    "filtered_train = filter_single_label(\"train\")\n",
    "filtered_validation = filter_single_label(\"validation\")\n",
    "filtered_test = filter_single_label(\"test\")\n",
    "\n",
    "# For statistics\n",
    "filtered_train_labels = [sample[\"label\"] for sample in filtered_train]\n",
    "class_counter = Counter(filtered_train_labels)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Save the three filtered splits as JSONL\n",
    "# -------------------------------\n",
    "save_dir = Path(\"..\") / \"data\" / \"splits_single_label\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_jsonl(data, name):\n",
    "    with open(save_dir / f\"{name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "save_jsonl(filtered_train, \"train\")\n",
    "save_jsonl(filtered_validation, \"validation\")\n",
    "save_jsonl(filtered_test, \"test\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Output some confirmation\n",
    "# -------------------------------\n",
    "{\n",
    "    \"train_single_label_count\": len(filtered_train),\n",
    "    \"validation_single_label_count\": len(filtered_validation),\n",
    "    \"test_single_label_count\": len(filtered_test),\n",
    "    \"train_class_distribution\": dict(sorted(class_counter.items()))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Agmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART paraphraser already exists at d:\\Documents\\FYP\\emotion-retrieval-embeddings\\models\\bart_paraphraser. Skipping download and save.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Compute correct save path\n",
    "# -------------------------------\n",
    "# Go up one level from notebook folder\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "model_save_path = os.path.join(project_root, \"models\", \"bart_paraphraser\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Check if model already exists\n",
    "# -------------------------------\n",
    "if os.path.exists(model_save_path) and os.path.isdir(model_save_path):\n",
    "    print(f\"BART paraphraser already exists at {model_save_path}. Skipping download and save.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Model not found at {model_save_path}. Downloading and saving...\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Download and save\n",
    "    # -------------------------------\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"eugenesiow/bart-paraphrase\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"eugenesiow/bart-paraphrase\")\n",
    "\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "    print(f\"Model successfully saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minority classes (less than 300 samples): [12, 23, 21, 16, 19]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define minority classes\n",
    "minority_class_ids = [class_id for class_id, count in class_counter.items() if count < 300]\n",
    "\n",
    "print(f\"Minority classes (less than 300 samples): {minority_class_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full agmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minority Classes to Augment: [12, 23, 21, 16, 19]\n",
      "\n",
      "Samples and paraphrases needed per minority class:\n",
      "Class ID   Samples to Generate  Paraphrases per Original Sample\n",
      "12         97                   1                             \n",
      "23         212                  3                             \n",
      "21         249                  5                             \n",
      "16         261                  7                             \n",
      "19         215                  3                             \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Find minority classes\n",
    "# -------------------------------\n",
    "\n",
    "# Assume:\n",
    "# - `filtered_labels` = list of ground-truth labels after filtering (single-label)\n",
    "# - `target_samples_per_class` = target number of samples per class\n",
    "\n",
    "target_samples_per_class = 300\n",
    "num_classes = 28\n",
    "\n",
    "# Find classes that are minority (less than target)\n",
    "minority_classes = [class_id for class_id, count in class_counter.items() if count < target_samples_per_class]\n",
    "\n",
    "print(\"Minority Classes to Augment:\", minority_classes)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Calculate generation goals\n",
    "# -------------------------------\n",
    "\n",
    "# For each minority class:\n",
    "# - How many samples to generate\n",
    "# - How many paraphrases needed per original sample\n",
    "class_to_generation_goal = {}\n",
    "class_to_paraphrase_per_sample = {}\n",
    "\n",
    "for class_id in minority_classes:\n",
    "    current_count = class_counter[class_id]\n",
    "    needed_count = target_samples_per_class - current_count\n",
    "\n",
    "    # Number of paraphrases needed per sample\n",
    "    # r_c = ceil(needed / existing)\n",
    "    paraphrases_per_sample = math.ceil(needed_count / current_count)\n",
    "\n",
    "    class_to_generation_goal[class_id] = needed_count\n",
    "    class_to_paraphrase_per_sample[class_id] = paraphrases_per_sample\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Print planning table\n",
    "# -------------------------------\n",
    "\n",
    "print(\"\\nSamples and paraphrases needed per minority class:\")\n",
    "print(\"{:<10} {:<20} {:<30}\".format(\"Class ID\", \"Samples to Generate\", \"Paraphrases per Original Sample\"))\n",
    "\n",
    "for class_id in minority_classes:\n",
    "    print(\"{:<10} {:<20} {:<30}\".format(\n",
    "        class_id,\n",
    "        class_to_generation_goal[class_id],\n",
    "        class_to_paraphrase_per_sample[class_id]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_text(text, num_return_sequences, paraphraser_pipeline):\n",
    "    \"\"\"\n",
    "    Generate paraphrases for a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to paraphrase.\n",
    "        num_return_sequences (int): How many paraphrased versions to generate.\n",
    "        paraphraser_pipeline (transformers.Pipeline): A HuggingFace pipeline for text2text-generation.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of generated paraphrased texts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate paraphrases\n",
    "        outputs = paraphraser_pipeline(\n",
    "            text,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_return_sequences + 4,  # +4 beams to increase diversity\n",
    "            max_length=128,\n",
    "            do_sample=False  # Deterministic decoding (no randomness)\n",
    "        )\n",
    "\n",
    "        # Extract generated texts\n",
    "        paraphrased_texts = [o['generated_text'] for o in outputs]\n",
    "        return paraphrased_texts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Paraphrasing failed: {e}\")\n",
    "        return []  # Return empty list if something goes wrong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load paraphraser model\n",
    "paraphraser = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"eugenesiow/bart-paraphrase\",\n",
    "    device=0  # or device=-1 if you are on CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes selected for augmentation:\n",
      "Class 12: 203 → 300 using 1x per sample\n",
      "Class 23: 88 → 300 using 3x per sample\n",
      "Class 21: 51 → 300 using 5x per sample\n",
      "Class 16: 39 → 300 using 7x per sample\n",
      "Class 19: 85 → 300 using 3x per sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Augmenting Class 12 ---\n",
      "Samples available: 203, generating 1 per sample → 203 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Augmenting Class 23 ---\n",
      "Samples available: 88, generating 3 per sample → 264 total\n",
      "\n",
      "--- Augmenting Class 21 ---\n",
      "Samples available: 51, generating 5 per sample → 255 total\n",
      "\n",
      "--- Augmenting Class 16 ---\n",
      "Samples available: 39, generating 7 per sample → 273 total\n",
      "\n",
      "--- Augmenting Class 19 ---\n",
      "Samples available: 85, generating 3 per sample → 255 total\n",
      "Augmentation complete. Total new augmented samples: 1250\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load filtered training data\n",
    "# -------------------------------\n",
    "train_path = Path(\"..\") / \"data\" / \"splits_single_label\" / \"train.jsonl\"\n",
    "\n",
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    filtered_train = [json.loads(line) for line in f]\n",
    "\n",
    "filtered_train_labels = [sample[\"label\"] for sample in filtered_train]\n",
    "train_class_counter = Counter(filtered_train_labels)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Determine augmentation plan\n",
    "# -------------------------------\n",
    "target_samples_per_class = 300\n",
    "minority_classes = [c for c, count in train_class_counter.items() if count < target_samples_per_class]\n",
    "\n",
    "class_to_paraphrase_per_sample = {\n",
    "    c: math.ceil((target_samples_per_class - train_class_counter[c]) / train_class_counter[c])\n",
    "    for c in minority_classes\n",
    "}\n",
    "\n",
    "print(\"\\nClasses selected for augmentation:\")\n",
    "for c in minority_classes:\n",
    "    print(f\"Class {c}: {train_class_counter[c]} → {target_samples_per_class} using {class_to_paraphrase_per_sample[c]}x per sample\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load paraphraser\n",
    "# -------------------------------\n",
    "paraphraser = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"eugenesiow/bart-paraphrase\",\n",
    "    device=0  # or -1 for CPU\n",
    ")\n",
    "\n",
    "def paraphrase_text(text, num_return_sequences, paraphraser_pipeline):\n",
    "    try:\n",
    "        outputs = paraphraser_pipeline(\n",
    "            text,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=num_return_sequences + 4,\n",
    "            max_length=128,\n",
    "            do_sample=False\n",
    "        )\n",
    "        return [o['generated_text'] for o in outputs]\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Skipped due to: {e}\")\n",
    "        return []\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Perform augmentation\n",
    "# -------------------------------\n",
    "augmented_raw_samples = []\n",
    "\n",
    "for class_id in minority_classes:\n",
    "    print(f\"\\n--- Augmenting Class {class_id} ---\")\n",
    "    original_samples = [s for s in filtered_train if s[\"label\"] == class_id]\n",
    "    r_c = class_to_paraphrase_per_sample[class_id]\n",
    "\n",
    "    print(f\"Samples available: {len(original_samples)}, generating {r_c} per sample → {len(original_samples) * r_c} total\")\n",
    "\n",
    "    for sample in original_samples:\n",
    "        raw_text = sample[\"text\"]\n",
    "        paraphrased = paraphrase_text(raw_text, r_c, paraphraser)\n",
    "\n",
    "        for p_text in paraphrased:\n",
    "            augmented_raw_samples.append({\n",
    "                \"text\": p_text,\n",
    "                \"label\": class_id\n",
    "            })\n",
    "\n",
    "print(f\"Augmentation complete. Total new augmented samples: {len(augmented_raw_samples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Viewing 8 randomly picked augmented samples per minority class ===\n",
      "\n",
      "--- Class 12 ---\n",
      "Sample 1: I'm so uncomfortable.\n",
      "Sample 2: *people’s ..this is awkward\n",
      "Sample 3: It's embarrassing to share a license with them, that's for sure.\n",
      "Sample 4: *concussion* *Trophy system kills it* *Another concussion* * Trophy System kills it * \"Yo guys watch me throw this cluster\" Honestly embarrassing \n",
      "Sample 5: Please tell me you were buying something extremely embarassing!\n",
      "Sample 6: Nobody said they don't have the right. It's still embarrassing.\n",
      "Sample 7: [NAME] type awkwardness that some thots go nuts for.\n",
      "Sample 8: Those women's marches are honestly embarrasing. Most of the people there only seem to be feminists because it's trendy.\n",
      "\n",
      "--- Class 16 ---\n",
      "Sample 1: Even today, many years later, that still gives me chills. RIP\n",
      "Sample 2: Well that’s not what was said he said drugs that kill you quicker than your meant to die, nicotine fits that description.\n",
      "Sample 3: They really weren't people, they were slavers. Civilized people should have only one answer to that sort of behaviour: violence and death.\n",
      "Sample 4: Youve never played against a good support player then\n",
      "Sample 5: youve never played against a good support then\n",
      "Sample 6: Well that’s not what was said he said drugs that kill you faster than your meant to die. Nicotine fits that description.\n",
      "Sample 7: Ah, this may explain all the untimely heart attack deaths we hear about for those who oppose the state or whatnot.\n",
      "Sample 8: [NAME] Gun crimes are pretty much unheard of on that side of the pond, isn't it?\n",
      "\n",
      "--- Class 19 ---\n",
      "Sample 1: I'm hoping it's more statistical noise at this point still, but it's definitely worrying.\n",
      "Sample 2: Classic Trap Game... I'm nervous. Hope our boys aren't hungover.\n",
      "Sample 3: I am also anxious that people will be angry or surprised or upset for me never telling people about this before.\n",
      "Sample 4: I couldn't reply to the message I received the other day.\n",
      "Sample 5: Now I'm worried that cat will get salmonella :(\n",
      "Sample 6: I gotta say, that was a well fought period. I was worried a couple times...\n",
      "Sample 7: I already called the county I got married in trying to find out about it but they had nothing on either of us about a divorce.. Im so anxious..\n",
      "Sample 8: JFC, that is depressing.\n",
      "\n",
      "--- Class 21 ---\n",
      "Sample 1: My pride. I am like a roach, unsightly to look at but determined to survive.\n",
      "Sample 2: Everyone hates these posts but when we do post them it's because we are proud of the work we do. Congrats!\n",
      "Sample 3: Not surprised by rudi, the man has been a beast since we got him.\n",
      "Sample 4: I'm proud that this is the first response that I see.\n",
      "Sample 5: I'm so glad they remastered that game, I'm having a blast playing it again.\n",
      "Sample 6: In China, innovation means \"I pridefully made this.\"\n",
      "Sample 7: New Year, New You. I'm proud of you.\n",
      "Sample 8: In my school we have specific councilors for the students matched alphabetically. Mine just sucks.\n",
      "\n",
      "--- Class 23 ---\n",
      "Sample 1: No one comes here it's cool\n",
      "Sample 2: Now let's sit down, relax and have a nice cool glass of turnip juice.\n",
      "Sample 3: At least she's not a dirty hamster.\n",
      "Sample 4: Sport shooting can be done with air guns. Problem solved.\n",
      "Sample 5: Finally! Somebody who legitimately hates planet Jupiter!\n",
      "Sample 6: Dude was okay! Oh thank god, I figured at best paralyzed from waist down.\n",
      "Sample 7: Glad your SO was helpful and hope you are better.\n",
      "Sample 8: I heard someone say one... `` everywhere is better but Perth is the best! Glad you enjoyed your holiday!\n"
     ]
    }
   ],
   "source": [
    "# View random samples per class\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class_to_augmented = defaultdict(list)\n",
    "\n",
    "for sample in augmented_raw_samples:\n",
    "    class_to_augmented[sample['label']].append(sample['text'])\n",
    "\n",
    "print(\"\\n=== Viewing 8 randomly picked augmented samples per minority class ===\")\n",
    "\n",
    "for class_id in sorted(minority_classes):\n",
    "    print(f\"\\n--- Class {class_id} ---\")\n",
    "    samples = random.sample(class_to_augmented[class_id], k=min(8, len(class_to_augmented[class_id])))\n",
    "    for i, s in enumerate(samples, 1):\n",
    "        print(f\"Sample {i}: {s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge the original + augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged total: 46696 (Train: 37558, Val: 4548, Test: 4590)\n",
      "Merged dataset with splits saved to: D:\\Documents\\FYP\\emotion-retrieval-embeddings\\data\\augmented_single_label.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load validation and test splits\n",
    "# -------------------------------\n",
    "val_path = Path(\"..\") / \"data\" / \"splits_single_label\" / \"validation.jsonl\"\n",
    "test_path = Path(\"..\") / \"data\" / \"splits_single_label\" / \"test.jsonl\"\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "filtered_validation = load_jsonl(val_path)\n",
    "filtered_test = load_jsonl(test_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Annotate splits and merge\n",
    "# -------------------------------\n",
    "\n",
    "# Add split tags\n",
    "train_all = [{\"text\": s[\"text\"], \"label\": s[\"label\"], \"split\": \"train\"} for s in filtered_train + augmented_raw_samples]\n",
    "validation_all = [{\"text\": s[\"text\"], \"label\": s[\"label\"], \"split\": \"validation\"} for s in filtered_validation]\n",
    "test_all = [{\"text\": s[\"text\"], \"label\": s[\"label\"], \"split\": \"test\"} for s in filtered_test]\n",
    "\n",
    "merged_dataset = train_all + validation_all + test_all\n",
    "\n",
    "print(f\"Merged total: {len(merged_dataset)} (Train: {len(train_all)}, Val: {len(validation_all)}, Test: {len(test_all)})\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Save as JSONL\n",
    "# -------------------------------\n",
    "save_path = Path(\"..\") / \"data\" / \"augmented_single_label.jsonl\"\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sample in merged_dataset:\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Merged dataset with splits saved to: {save_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a balanced batch sampler for single-label samples\n",
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class UniformBalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, num_classes, labels):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.labels = labels\n",
    "\n",
    "        # Build mapping: class -> list of indices\n",
    "        self.class_to_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            self.class_to_indices[label].append(idx)\n",
    "\n",
    "        self.available_classes = [c for c in range(self.num_classes) if len(self.class_to_indices[c]) > 0]\n",
    "\n",
    "        if self.batch_size < len(self.available_classes):\n",
    "            raise ValueError(f\"Batch size must be >= number of available classes ({len(self.available_classes)}). Got {self.batch_size}.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            batch_indices = []\n",
    "\n",
    "            # Sample one from each available class\n",
    "            for class_id in self.available_classes:\n",
    "                candidates = self.class_to_indices[class_id]\n",
    "                if candidates:\n",
    "                    selected = random.choice(candidates)\n",
    "                    batch_indices.append(selected)\n",
    "\n",
    "            # Fill remaining slots randomly\n",
    "            while len(batch_indices) < self.batch_size:\n",
    "                class_id = random.choice(self.available_classes)\n",
    "                candidates = self.class_to_indices[class_id]\n",
    "                selected = random.choice(candidates)\n",
    "                batch_indices.append(selected)\n",
    "\n",
    "            random.shuffle(batch_indices)\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1000000  # or some large number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BSCLossSingleLabel(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(BSCLossSingleLabel, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Tensor of shape (batch_size, hidden_dim) - embeddings\n",
    "            labels: Tensor of shape (batch_size,) - single integer label per sample\n",
    "        \"\"\"\n",
    "        device = features.device\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Normalize features to unit hypersphere\n",
    "        features = F.normalize(features, p=2, dim=1)\n",
    "\n",
    "        # Compute similarity matrix (batch_size x batch_size)\n",
    "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "\n",
    "        # Positive mask: label[i] == label[j], and i != j\n",
    "        labels = labels.view(-1, 1)  # shape (batch_size, 1)\n",
    "        positive_mask = (labels == labels.T).float()  # (batch_size, batch_size)\n",
    "        diag_mask = torch.eye(batch_size, device=device)\n",
    "        positive_mask = positive_mask * (1 - diag_mask)  # remove diagonal\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            pos_indices = positive_mask[i].nonzero(as_tuple=False).squeeze(1)\n",
    "\n",
    "            if len(pos_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            numerator = torch.exp(sim_matrix[i, pos_indices])\n",
    "\n",
    "            denominator = 0.0\n",
    "            for c in torch.unique(labels):\n",
    "                class_indices = (labels.squeeze() == c).nonzero(as_tuple=False).squeeze(1)\n",
    "                class_indices = class_indices[class_indices != i]\n",
    "\n",
    "                if len(class_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                class_sims = torch.exp(sim_matrix[i, class_indices])\n",
    "                class_sum = class_sims.sum()\n",
    "                class_sum = class_sum / len(class_indices)\n",
    "\n",
    "                denominator += class_sum\n",
    "\n",
    "            loss_i = - torch.mean(torch.log(numerator / denominator))\n",
    "            losses.append(loss_i)\n",
    "\n",
    "        if len(losses) == 0:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "\n",
    "        return torch.mean(torch.stack(losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test my loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Detailed BSCL Diagnostic ===\n",
      "\n",
      "Similarity Matrix:\n",
      "tensor([[10.0000, -1.2799, -4.4199,  7.7487, -3.4071, -0.1243, -8.3086,  7.1075],\n",
      "        [-1.2799, 10.0000,  1.6897,  0.6522, -2.5841,  1.6311,  5.5117, -3.7046],\n",
      "        [-4.4199,  1.6897, 10.0000, -6.9804, -6.4275, -6.7476,  3.7009, -0.7663],\n",
      "        [ 7.7487,  0.6522, -6.9804, 10.0000,  0.5539,  5.7648, -4.0981,  6.2141],\n",
      "        [-3.4071, -2.5841, -6.4275,  0.5539, 10.0000,  5.6240,  1.7582, -4.3310],\n",
      "        [-0.1243,  1.6311, -6.7476,  5.7648,  5.6240, 10.0000,  3.1521,  0.3289],\n",
      "        [-8.3086,  5.5117,  3.7009, -4.0981,  1.7582,  3.1521, 10.0000, -5.4888],\n",
      "        [ 7.1075, -3.7046, -0.7663,  6.2141, -4.3310,  0.3289, -5.4888, 10.0000]])\n",
      "\n",
      "Positive Mask (1 = same class, 0 = other):\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "\n",
      "Class-wise indices:\n",
      "Class 0: indices [0, 1]\n",
      "Class 1: indices [2, 3]\n",
      "Class 2: indices [4, 5]\n",
      "Class 3: indices [6, 7]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def print_sim_matrix_info(features, labels, temperature=0.1):\n",
    "    print(\"\\n=== Detailed BSCL Diagnostic ===\")\n",
    "    features = F.normalize(features, p=2, dim=1)\n",
    "    sim_matrix = torch.matmul(features, features.T) / temperature\n",
    "\n",
    "    print(\"\\nSimilarity Matrix:\")\n",
    "    print(sim_matrix)\n",
    "\n",
    "    labels = labels.view(-1, 1)\n",
    "    batch_size = labels.size(0)\n",
    "    pos_mask = (labels == labels.T).float() - torch.eye(batch_size)\n",
    "    print(\"\\nPositive Mask (1 = same class, 0 = other):\")\n",
    "    print(pos_mask)\n",
    "\n",
    "    print(\"\\nClass-wise indices:\")\n",
    "    for c in torch.unique(labels):\n",
    "        indices = (labels.squeeze() == c).nonzero(as_tuple=False).squeeze(1)\n",
    "        print(f\"Class {c.item()}: indices {indices.tolist()}\")\n",
    "\n",
    "    return sim_matrix, pos_mask\n",
    "\n",
    "features = F.normalize(torch.randn(8, 5), dim=1)  # Random 8x5 features\n",
    "labels = torch.tensor([0, 0, 1, 1, 2, 2, 3, 3])   # 4 classes, 2 samples each\n",
    "\n",
    "sim_matrix, pos_mask = print_sim_matrix_info(features, labels, temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Similarity Matrix ===\n",
      "tensor([[ 10.0000,   9.9400,   0.0000,   0.0000,   0.0000,   1.1000, -10.0000,\n",
      "          -9.9400],\n",
      "        [  9.9400,  10.0000,   1.1000,   1.1000,   0.0000,   1.1000,  -9.9400,\n",
      "         -10.0000],\n",
      "        [  0.0000,   1.1000,  10.0000,   9.9400,   0.0000,   0.0000,   0.0000,\n",
      "          -1.1000],\n",
      "        [  0.0000,   1.1000,   9.9400,  10.0000,   1.1000,   1.1000,   0.0000,\n",
      "          -1.1000],\n",
      "        [  0.0000,   0.0000,   0.0000,   1.1000,  10.0000,   9.9400,   0.0000,\n",
      "           0.0000],\n",
      "        [  1.1000,   1.1000,   0.0000,   1.1000,   9.9400,  10.0000,  -1.1000,\n",
      "          -1.1000],\n",
      "        [-10.0000,  -9.9400,   0.0000,   0.0000,   0.0000,  -1.1000,  10.0000,\n",
      "           9.9400],\n",
      "        [ -9.9400, -10.0000,  -1.1000,  -1.1000,   0.0000,  -1.1000,   9.9400,\n",
      "          10.0000]])\n",
      "\n",
      "=== Positive Mask ===\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "\n",
      "=== Positive Indices per Sample ===\n",
      "Sample 0: Positives -> [1], Class: 0\n",
      "Sample 1: Positives -> [0], Class: 0\n",
      "Sample 2: Positives -> [3], Class: 1\n",
      "Sample 3: Positives -> [2], Class: 1\n",
      "Sample 4: Positives -> [5], Class: 2\n",
      "Sample 5: Positives -> [4], Class: 2\n",
      "Sample 6: Positives -> [7], Class: 3\n",
      "Sample 7: Positives -> [6], Class: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a dummy 8-vector batch with 4 classes (2 per class)\n",
    "x = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],    # Class 0\n",
    "    [0.9, 0.1, 0.0],    # Class 0\n",
    "    [0.0, 1.0, 0.0],    # Class 1\n",
    "    [0.0, 0.9, 0.1],    # Class 1\n",
    "    [0.0, 0.0, 1.0],    # Class 2\n",
    "    [0.1, 0.0, 0.9],    # Class 2\n",
    "    [-1.0, 0.0, 0.0],   # Class 3\n",
    "    [-0.9, -0.1, 0.0],  # Class 3\n",
    "])\n",
    "y = torch.tensor([0, 0, 1, 1, 2, 2, 3, 3])\n",
    "\n",
    "# Normalize\n",
    "x_norm = F.normalize(x, p=2, dim=1)\n",
    "sim_matrix = torch.matmul(x_norm, x_norm.T) / 0.1\n",
    "\n",
    "# Compute positive mask\n",
    "labels = y.view(-1, 1)\n",
    "positive_mask = (labels == labels.T).float()\n",
    "diag = torch.eye(len(y))\n",
    "positive_mask = positive_mask * (1 - diag)\n",
    "\n",
    "# Print diagnostics\n",
    "print(\"=== Similarity Matrix ===\")\n",
    "print(sim_matrix.round(decimals=2))  # FIXED: use decimals=2 instead of positional arg\n",
    "\n",
    "print(\"\\n=== Positive Mask ===\")\n",
    "print(positive_mask)\n",
    "\n",
    "print(\"\\n=== Positive Indices per Sample ===\")\n",
    "for i in range(len(y)):\n",
    "    pos_indices = (positive_mask[i] > 0).nonzero(as_tuple=False).squeeze(1).tolist()\n",
    "    print(f\"Sample {i}: Positives -> {pos_indices}, Class: {y[i].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9187521934509277"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# Simulate realistic batch of features from a small model\n",
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "feature_dim = 64\n",
    "num_classes = 8\n",
    "\n",
    "# Randomly generate normalized feature vectors\n",
    "features = F.normalize(torch.randn(batch_size, feature_dim), p=2, dim=1)\n",
    "\n",
    "# Randomly assign labels (integers from 0 to num_classes-1)\n",
    "labels = torch.randint(low=0, high=num_classes, size=(batch_size,))\n",
    "\n",
    "# Create criterion and compute loss\n",
    "criterion = BSCLossSingleLabel(temperature=0.1)\n",
    "loss_value = criterion(features, labels)\n",
    "\n",
    "loss_value.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning plan\n",
    "1.  Stage 1 creates a strong \"backbone\" embedding space, where emotions are cleanly separated. Filter GoEmotions to only single-label sentences\n",
    "2. Stage 2 teaches the model the complex, messy mixtures that real human feelings exhibit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a815104ab4a14d71b7c3215d867249be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Correct path to the actual file\n",
    "dataset_path = Path(\"..\") / \"data\" / \"augmented_single_label.jsonl\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=str(dataset_path), split=\"train\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

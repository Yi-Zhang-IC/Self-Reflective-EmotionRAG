{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import math\n",
    "import json \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import openai\n",
    "from random import randint\n",
    "from typing import List\n",
    "import tqdm\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 characters.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = \"../../database\"\n",
    "\n",
    "# List all character subfolders\n",
    "character_names = [name for name in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, name))]\n",
    "\n",
    "# Load all raw_paragraphs.json files\n",
    "all_raw_paragraphs = {}\n",
    "for character in character_names:\n",
    "    path = os.path.join(BASE_DIR, character, \"raw_paragraphs.json\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_raw_paragraphs[character] = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(all_raw_paragraphs)} characters.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_path = \"../../models/roberta-base-go_emotions\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, truncation=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Instantiate the OpenAI client properly\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def generate_memories_gpt(paragraph_text, paragraph_index, character_name, model=\"gpt-4.1-nano\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Calls the OpenAI API to extract emotionally rich memory events from a paragraph.\n",
    "\n",
    "    Args:\n",
    "        paragraph_text (str): A paragraph from the Fandom biography.\n",
    "        paragraph_index (int): Index of the paragraph in source file.\n",
    "        model (str): Model name to use.\n",
    "        temperature (float): Sampling temperature.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: List of memory objects with 'text' and 'source_paragraph_index'.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are converting a paragraph from {character_name}'s biography into a more compact and emotional stoty.\n",
    "\n",
    "Instructions:\n",
    "- convert to 1-2 short story.\n",
    "- Always use the character's name in each story - avoid using 'he' or 'she'.\n",
    "- Each must be self-contained and emotionally vivid.\n",
    "- Write in third person, and keep each under 64 words.\n",
    "- Return a JSON list of: {{\"text\": ..., \"source_paragraph_index\": {paragraph_index}}}\n",
    "\n",
    "Paragraph:\n",
    "\\\"\\\"\\\"{paragraph_text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "\n",
    "    # Parse and return list of memory entries\n",
    "    try:\n",
    "        return json.loads(response.choices[0].message.content.strip())\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ö†Ô∏è GPT returned invalid JSON for paragraph index\", paragraph_index)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paragraph 0 ---\n",
      "Severus Snape was born 9 January 1960 to Tobias Snape, an abusive Muggle, and Eileen Prince, a neglectful pure-blood witch.[23] He began to identify with his mother's family and created a secret nickname from his mother's maiden name, calling himself the \"Half-Blood Prince\". It is implied that Severus was friendless and uncared for by his parents. This lack of care largely shaped Severus's bitter disposition and cruel behaviour later in his life.\n",
      "\n",
      "--- Paragraph 1 ---\n",
      "Severus grew up at Spinner's End, a shabby suburb of Cokeworth.[24] This area of town was near a dirty river and full of dilapidated houses, disused factories and broken down street lamps. Through the rest of his life, Severus continued to return there when he was not at school. The young Severus is depicted as being unwashed and wearing ill-fitting clothes \"that were so mismatched that it looked deliberate\". As a child, Severus was neglected and his parents often fought with one another. He could not wait to leave for Hogwarts at the end of the summer.[7] Despite this, one of his favourite foods from his childhood was Holiday Blancmange, which persisted into his adult years.[25]\n",
      "\n",
      "--- Paragraph 2 ---\n",
      "Lily Evans and her family lived in the same town, close to Spinner's End. After watching her for some time, Severus noticed her evident magical abilities and began making friendly overtures. The two bonded quickly and it appears that he was very interested in Lily right from the beginning, though she only regarded him as a good friend. During this time he also developed a contempt towards her older sister, Petunia, who made disparaging comments about his clothes and residence.[7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Snape's raw paragraphs\n",
    "with open(\"../../database/severus_snape/raw_paragraphs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    snape_paragraphs = json.load(f)\n",
    "\n",
    "# Preview top 3 paragraphs\n",
    "for i in range(3):\n",
    "    print(f\"--- Paragraph {i} ---\\n{snape_paragraphs[i]['raw_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ (71 tokens): Severus Snape, born to an abusive Muggle father and a neglectful witch mother, grew up unloved and alone. Embracing his mother's bloodline, he secretly called himself the 'Half-Blood Prince,' his bitterness deepening with every moment of neglect and pain. His lonely childhood forged a heart hardened by pain and cruelty.\n",
      "\n",
      "üîπ (76 tokens): Severus Snape grew up in Spinner's End, a bleak neighborhood haunted by neglect. Surrounded by broken homes and a dirty river, he longed for escape. The fights at home made him yearn for Hogwarts, where he found solace. Yet, even as an adult, the taste of Holiday Blancmange reminded him of childhood's fleeting comfort.\n",
      "\n",
      "üîπ (68 tokens): Severus Snape watched Lily Evans with admiration, sensing her hidden magic. Their bond blossomed quickly, filling him with hope and affection. Yet Lily saw him only as a friend, while he harbored feelings that ran deep, longing for a connection that seemed just out of reach. His heart ached with unspoken love.\n",
      "\n",
      "üîπ (69 tokens): Severus Snape's eyes darkened as he observed Petunia's spiteful words about his humble home. Behind his facade, pain simmered‚Äîdisdain for her cruelty and a longing to be accepted. Lily's kindness contrasted sharply with Petunia's contempt, fueling Snape's quiet bitterness and unspoken yearning for something more.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run memory generation on top 3\n",
    "test_memories = []\n",
    "for i in range(3):\n",
    "    para = snape_paragraphs[i]\n",
    "    output = generate_memories_gpt(para[\"raw_text\"], para[\"paragraph_index\"], \"Severus Snape\", model=\"gpt-4.1-nano\", temperature=0.7)\n",
    "    test_memories.extend(output)\n",
    "\n",
    "# Print results\n",
    "for mem in test_memories:\n",
    "    print(f\"üîπ ({count_tokens(mem['text'])} tokens): {mem['text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Minerva Mcgonagall: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [03:11<00:00,  1.30s/it]\n",
      "Processing Harry Potter:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 386/391 [09:13<00:06,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è GPT returned invalid JSON for paragraph index 385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Harry Potter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [09:20<00:00,  1.43s/it]\n",
      "Processing Ron Weasley:  11%|‚ñà         | 14/133 [00:15<01:59,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error on paragraph 14 (attempt 1): string indices must be integers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Ron Weasley:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 102/133 [02:21<00:41,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error on paragraph 102 (attempt 1): string indices must be integers\n",
      "‚ö†Ô∏è Error on paragraph 102 (attempt 2): string indices must be integers\n",
      "‚ö†Ô∏è Error on paragraph 102 (attempt 3): string indices must be integers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Ron Weasley:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 103/133 [02:31<01:53,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed after 3 attempts on paragraph 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Ron Weasley: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133/133 [03:14<00:00,  1.46s/it]\n",
      "Processing Luna Lovegood: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:08<00:00,  1.25s/it]\n",
      "Processing Albus Dumbledore:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 89/164 [01:56<01:44,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è GPT returned invalid JSON for paragraph index 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Albus Dumbledore:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 101/164 [02:16<02:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è GPT returned invalid JSON for paragraph index 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Albus Dumbledore: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [03:44<00:00,  1.37s/it]\n",
      "Processing Severus Snape: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [03:09<00:00,  1.36s/it]\n",
      "Processing Hermione Granger:  15%|‚ñà‚ñç        | 21/142 [00:23<02:30,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è GPT returned invalid JSON for paragraph index 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Hermione Granger:  23%|‚ñà‚ñà‚ñé       | 33/142 [00:38<02:24,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è GPT returned invalid JSON for paragraph index 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Hermione Granger: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 142/142 [03:06<00:00,  1.31s/it]\n",
      "Processing Draco Malfoy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [01:21<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_DIR = \"../../database\"\n",
    "character_dirs = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, truncation=False))\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "SLEEP_BETWEEN_RETRIES = 2  # seconds\n",
    "\n",
    "for char_dir in character_dirs:\n",
    "    char_name = char_dir.replace(\"_\", \" \").title()\n",
    "    raw_path = os.path.join(BASE_DIR, char_dir, \"raw_paragraphs.json\")\n",
    "    memory_path = os.path.join(BASE_DIR, char_dir, \"memory.json\")\n",
    "\n",
    "    if not os.path.exists(raw_path):\n",
    "        continue\n",
    "\n",
    "    # Load paragraph source\n",
    "    with open(raw_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        paragraphs = json.load(f)\n",
    "\n",
    "    # Load existing memory if resuming\n",
    "    if os.path.exists(memory_path):\n",
    "        with open(memory_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_memories = json.load(f)\n",
    "        existing_indices = {m[\"source_paragraph_index\"] for m in all_memories}\n",
    "    else:\n",
    "        all_memories = []\n",
    "        existing_indices = set()\n",
    "\n",
    "    for entry in tqdm(paragraphs, desc=f\"Processing {char_name}\"):\n",
    "        para_idx = entry[\"paragraph_index\"]\n",
    "        para_text = entry[\"raw_text\"]\n",
    "\n",
    "        if para_idx in existing_indices:\n",
    "            continue  # Skip already processed\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                mems = generate_memories_gpt(para_text, para_idx, character_name=char_name)\n",
    "                # Check token limits\n",
    "                mems = [m for m in mems if 50 <= count_tokens(m[\"text\"]) <= 128]\n",
    "                all_memories.extend(mems)\n",
    "\n",
    "                # Save after each success\n",
    "                with open(memory_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(all_memories, f, indent=2, ensure_ascii=False)\n",
    "                break  # Success ‚Üí break retry loop\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error on paragraph {para_idx} (attempt {attempt}): {e}\")\n",
    "                time.sleep(SLEEP_BETWEEN_RETRIES)\n",
    "\n",
    "                if attempt == MAX_RETRIES:\n",
    "                    print(f\"‚ùå Failed after {MAX_RETRIES} attempts on paragraph {para_idx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "char = \"ron_weasley\"\n",
    "paragraph_index = 102\n",
    "\n",
    "# Load raw paragraph\n",
    "with open(f\"../../database/{char}/raw_paragraphs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    paragraphs = json.load(f)\n",
    "\n",
    "target_paragraph = [p for p in paragraphs if p[\"paragraph_index\"] == paragraph_index][0]\n",
    "\n",
    "# Load existing memory\n",
    "with open(f\"../../database/{char}/memory.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    memories = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun GPT\n",
    "patched_mems = generate_memories_gpt(\n",
    "    paragraph_text=target_paragraph[\"raw_text\"],\n",
    "    paragraph_index=paragraph_index,\n",
    "    character_name=\"Ron Weasley\"\n",
    ")\n",
    "\n",
    "# Optional: filter by token length\n",
    "patched_mems = [m for m in patched_mems if 50 <= count_tokens(m[\"text\"]) <= 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Patched paragraph 102 for Ron Weasley.\n"
     ]
    }
   ],
   "source": [
    "# Add to memory list\n",
    "memories.extend(patched_mems)\n",
    "\n",
    "# Sort by paragraph index (optional but tidy)\n",
    "memories = sorted(memories, key=lambda x: x[\"source_paragraph_index\"])\n",
    "\n",
    "# Save updated memory\n",
    "with open(f\"../../database/{char}/memory.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(memories, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Patched paragraph {paragraph_index} for Ron Weasley.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ† Patching: luna_lovegood\n",
      "üîç Missing: [32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patching Luna Lovegood: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = \"../../database\"\n",
    "character_dirs = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]\n",
    "\n",
    "# Thresholds\n",
    "MAX_RETRIES = 3\n",
    "TOKEN_MIN = 50\n",
    "TOKEN_MAX = 128\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(path, obj):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def patch_memory(char):\n",
    "    print(f\"\\nüõ† Patching: {char}\")\n",
    "    \n",
    "    raw_path = os.path.join(BASE_DIR, char, \"raw_paragraphs.json\")\n",
    "    mem_path = os.path.join(BASE_DIR, char, \"memory.json\")\n",
    "    character_name = char.replace(\"_\", \" \").title()\n",
    "    \n",
    "    raw_paragraphs = load_json(raw_path)\n",
    "    raw_index_map = {p[\"paragraph_index\"]: p[\"raw_text\"] for p in raw_paragraphs}\n",
    "    raw_indices = set(raw_index_map.keys())\n",
    "\n",
    "    if os.path.exists(mem_path):\n",
    "        memories = load_json(mem_path)\n",
    "        mem_indices = {m[\"source_paragraph_index\"] for m in memories}\n",
    "    else:\n",
    "        memories = []\n",
    "        mem_indices = set()\n",
    "\n",
    "    missing_indices = sorted(raw_indices - mem_indices)\n",
    "    print(f\"üîç Missing: {missing_indices}\")\n",
    "\n",
    "    for idx in tqdm(missing_indices, desc=f\"Patching {character_name}\"):\n",
    "        para_text = raw_index_map[idx]\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                mems = generate_memories_gpt(para_text, idx, character_name)\n",
    "                filtered = [m for m in mems if TOKEN_MIN <= count_tokens(m[\"text\"]) <= TOKEN_MAX]\n",
    "                memories.extend(filtered)\n",
    "\n",
    "                # Save after each successful addition\n",
    "                save_json(mem_path, sorted(memories, key=lambda x: x[\"source_paragraph_index\"]))\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error on {char} paragraph {idx} (attempt {attempt}): {e}\")\n",
    "                time.sleep(2)\n",
    "                if attempt == MAX_RETRIES:\n",
    "                    print(f\"‚ùå Failed to patch {char} paragraph {idx} after {MAX_RETRIES} attempts.\")\n",
    "\n",
    "# Only patch characters with known missing data\n",
    "incomplete_info = {\n",
    "    \"luna_lovegood\": [32],\n",
    "}\n",
    "\n",
    "for char, missing in incomplete_info.items():\n",
    "    patch_memory(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minerva_mcgonagall: complete\n",
      "harry_potter: complete\n",
      "ron_weasley: complete\n",
      "luna_lovegood: complete\n",
      "albus_dumbledore: complete\n",
      "severus_snape: complete\n",
      "hermione_granger: complete\n",
      "draco_malfoy: complete\n",
      "\n",
      "üîç Summary:\n",
      "Checked 8 characters.\n",
      "All characters have complete memory coverage!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "BASE_DIR = \"../../database\"\n",
    "character_dirs = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "incomplete_characters = {}\n",
    "\n",
    "for char in character_dirs:\n",
    "    raw_path = os.path.join(BASE_DIR, char, \"raw_paragraphs.json\")\n",
    "    mem_path = os.path.join(BASE_DIR, char, \"memory.json\")\n",
    "\n",
    "    if not os.path.exists(raw_path):\n",
    "        print(f\"‚ö†Ô∏è No raw_paragraphs.json for {char}\")\n",
    "        continue\n",
    "\n",
    "    raw_paragraphs = load_json(raw_path)\n",
    "    raw_indices = {p[\"paragraph_index\"] for p in raw_paragraphs}\n",
    "\n",
    "    if not os.path.exists(mem_path):\n",
    "        print(f\"No memory.json for {char} ‚Äî all {len(raw_indices)} missing\")\n",
    "        incomplete_characters[char] = sorted(list(raw_indices))\n",
    "        continue\n",
    "\n",
    "    memories = load_json(mem_path)\n",
    "    mem_indices = {m[\"source_paragraph_index\"] for m in memories}\n",
    "\n",
    "    missing = raw_indices - mem_indices\n",
    "    if missing:\n",
    "        incomplete_characters[char] = sorted(list(missing))\n",
    "        missing_sorted = sorted(missing)\n",
    "        print(f\"{char}: {len(missing_sorted)} missing of {len(raw_indices)} paragraphs ‚Üí {missing_sorted[:5]}{'...' if len(missing_sorted) > 5 else ''}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"{char}: complete\")\n",
    "\n",
    "# Optional summary\n",
    "print(\"\\nüîç Summary:\")\n",
    "print(f\"Checked {len(character_dirs)} characters.\")\n",
    "if incomplete_characters:\n",
    "    print(f\"{len(incomplete_characters)} incomplete:\")\n",
    "    for char, indices in incomplete_characters.items():\n",
    "        print(f\" - {char}: {len(indices)} missing ‚Üí {indices[:5]}{'...' if len(indices) > 5 else ''}\")\n",
    "else:\n",
    "    print(\"All characters have complete memory coverage!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
